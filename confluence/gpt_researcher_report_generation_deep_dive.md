# GPT Researcher: Report Generation & Writing Deep Dive

This document details how GPT Researcher generates reports, where they are saved, and how to customize the output format and location.

---

## 1. High-Level Workflow

The report generation process is a distinct phase that happens **after** research (scraping & context aggregation) is complete.

1.  **Research Phase**: `conduct_research()` gathers context.
2.  **Generation Phase**: `write_report()` sends context + query to the LLM to write the report.
3.  **Output Phase**: `write_report()` returns a **string** (Markdown). Saving to a file is handled by the caller (e.g., `cli.py` or your custom script).

> **Key concept:** The `GPTResearcher` class **does not write files to disk** itself. It returns the report content as a string. The "saving to `outputs/`" behavior you see is implemented in the `cli.py` script, not the core library.

---

## 2. The Report Generation Logic

**File:** `gpt_researcher/skills/writer.py` -> `ReportGenerator.write_report()`

When `write_report()` is called:

1.  **Context Preparation**: It takes the accumulated context (from `conduct_research`) and optionally pre-generated images.
2.  **Prompt Selection**: It calls `gpt_researcher/actions/report_generation.py`, which selects a prompt from `gpt_researcher/prompts.py` based on `report_type` (e.g., "research_report", "detailed_report").
3.  **LLM Generation**: It sends the prompt + context to the LLM (via `create_chat_completion`).
4.  **Response**: The LLM streams back the report in **Markdown** format.

### Code Snippet (`actions/report_generation.py`)

```python
# The core generation call
report = await create_chat_completion(
    model=cfg.smart_llm_model,
    messages=[
        {"role": "system", "content": f"{agent_role_prompt}"},
        {"role": "user", "content": content}, # content includes instructions + context
    ],
    ...
)
return report # Returns a string
```

---

## 3. Where are reports written?

### In the CLI (`cli.py`)

If you run `python cli.py "query"`, the saving logic is hardcoded in the `main` function of `cli.py`:

```python
# gpt_researcher/cli.py

# 1. Get the string
report = await researcher.write_report()

# 2. Define path (Hardcoded to 'outputs/')
task_id = str(uuid4())
artifact_filepath = f"outputs/{task_id}.md"
os.makedirs("outputs", exist_ok=True)

# 3. Write to disk
with open(artifact_filepath, "w", encoding="utf-8") as f:
    f.write(report)
```

### In a Custom Script

If you are using GPT Researcher as a library, **you control where it saves**.

```python
from gpt_researcher import GPTResearcher
import asyncio

async def main():
    researcher = GPTResearcher(query="My Query", report_type="research_report")
    await researcher.conduct_research()
    
    # Get the report string
    report_content = await researcher.write_report()
    
    # SAVE WHEREVER YOU WANT
    with open("/my/custom/path/report.md", "w") as f:
        f.write(report_content)
        
    # OR send to an API
    # requests.post("https://api.example.com/reports", data={"content": report_content})

asyncio.run(main())
```

---

## 4. Output Formats (Markdown, PDF, Docx)

The LLM **always** produces Markdown. Other formats are generated by converting that Markdown file.

### How Format is Determined

1.  **Markdown**: Native output of the LLM.
2.  **PDF/Docx**: Generated via conversion tools in `backend/utils.py`.

In `cli.py`, flags determine which conversions to run:

```python
# cli.py

# Always saves Markdown first
# ...

# Convert to PDF
if not args.no_pdf:
    from backend.utils import write_md_to_pdf
    await write_md_to_pdf(report, task_id)

# Convert to Docx
if not args.no_docx:
    from backend.utils import write_md_to_word
    await write_md_to_word(report, task_id)
```

### Customizing Formatting

To change *how* the report looks (e.g., APA style, specific headers):
1.  **Modify Prompts**: Edit `gpt_researcher/prompts.py` to instruct the LLM (e.g., "Write in APA format...").
2.  **Custom Prompt**: Pass a `custom_prompt` argument to `write_report()`.

```python
await researcher.write_report(custom_prompt="Please write a report in APA format about...")
```

---

## 5. Summary & Customization Reference

| Feature | Controlled By | How to Customize |
|---|---|---|
| **Save Location** | The *caller* (e.g., `cli.py` or your script) | Modify `cli.py` or write your own script to save `report_content` anywhere. |
| **File Format** | `backend/utils.py` (converters) | The LLM outputs Markdown. Use `write_md_to_pdf` or `write_md_to_word` for others. |
| **Report Structure** | `gpt_researcher/prompts.py` | Edit the `generate_report_prompt` function in `prompts.py`. |
| **Tone/Style** | `tone` parameter | Pass `tone=Tone.Humorous` etc. to `GPTResearcher` constructor. |

### How to Refactor for API Callback

If you want to send the report to an API instead of a file:

1.  Create a custom script (don't use `cli.py`).
2.  Run `report = await researcher.write_report()`.
3.  Add your API call:
    ```python
    import requests
    requests.post("https://my-api.com/webhook", json={"report": report})
    ```
